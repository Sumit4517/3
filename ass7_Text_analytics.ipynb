{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba64e8d-ff97-4ae9-b7bd-571095ade4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from  nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717ceff6-4b62-4cf6-abba-1b606857d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\darsh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'months', '.']\n",
      "['Hello I am Gayatri Deshmukh.', 'I am from Nanded District.', 'I will be an Engineer in few months.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello I am Gayatri Deshmukh. I am from Nanded District. I will be an Engineer in few months.\"\n",
    "\n",
    "#Tokenization\n",
    "nltk.download('punkt')\n",
    "tokenized_words = word_tokenize(sentence)\n",
    "tokenized_sentences = sent_tokenize(sentence)\n",
    "\n",
    "print(tokenized_words)\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e6f3d6-02a6-4e19-b94a-29a1b697263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\darsh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclean version  ['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'months', '.']\n",
      "Clean Version ['Hello', 'I', 'Gayatri', 'Deshmukh', '.', 'I', 'Nanded', 'District', '.', 'I', 'Engineer', 'months', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Stop words removal\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "cleaned_token = []\n",
    "for i in tokenized_words:\n",
    "  if i not in stop_words:\n",
    "    cleaned_token.append(i)\n",
    "\n",
    "print(\"Unclean version \", tokenized_words)\n",
    "print(\"Clean Version\", cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb81d263-2bcb-4820-b467-07c8669f63ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'i', 'am', 'gayatri', 'deshmukh', '.', 'i', 'am', 'from', 'nand', 'district', '.', 'i', 'will', 'be', 'an', 'engin', 'in', 'few', 'month', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "stemmed_words = []\n",
    "for i in tokenized_words:\n",
    "    stemmed = snowball_stemmer.stem(i)\n",
    "    stemmed_words.append(stemmed)\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c95c4f17-ff29-4fd1-8dce-fa888c5b6494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\darsh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'month', '.']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = []\n",
    "for i in tokenized_words:\n",
    "    lemmatized = wordnet_lemmatizer.lemmatize(i)\n",
    "    lemmatized_words.append(lemmatized)\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4804ac3a-2f4b-4950-8548-2e4b71c97814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\darsh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('Gayatri', 'NNP'), ('Deshmukh', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), ('Nanded', 'NNP'), ('District', 'NNP'), ('.', '.'), ('I', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('an', 'DT'), ('Engineer', 'NNP'), ('in', 'IN'), ('few', 'JJ'), ('months', 'NNS'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "#Pos Tagging\n",
    "# dt - determinnant\n",
    "# NN - noun\n",
    "# In - prep / conjunc\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pos_tag = nltk.pos_tag(tokenized_words)\n",
    "\n",
    "print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1113e7fe-779c-42fd-8916-447156e6d051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Indexing:  {'good': 4, 'morning': 8, 'do': 1, 'daily': 0, 'exercise': 2, 'in': 6, 'the': 9, 'is': 7, 'for': 3, 'health': 5}\n",
      "tf-idf in matrix form: \n",
      " [[0.         0.         0.         0.         0.70710678 0.\n",
      "  0.         0.         0.70710678 0.        ]\n",
      " [0.44036207 0.44036207 0.3349067  0.         0.         0.\n",
      "  0.44036207 0.         0.3349067  0.44036207]\n",
      " [0.         0.         0.37302199 0.49047908 0.37302199 0.49047908\n",
      "  0.         0.49047908 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "d0 = \"Good Morning\"\n",
    "d1 = \"Do daily exercise in the morning \"\n",
    "d2 = \"exercise is good for health\"\n",
    "series = [d0, d1, d2]\n",
    "tfidf = TfidfVectorizer()\n",
    "result = tfidf.fit_transform(series)\n",
    "\n",
    "print(\"Word Indexing: \", tfidf.vocabulary_)\n",
    "print(\"tf-idf in matrix form: \\n\", result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb7d3f3-aa9d-44b7-9396-f357de1166ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
